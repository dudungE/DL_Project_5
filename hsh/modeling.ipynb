{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추후 로컬 gpu 사용하는 방법 연구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('39', 3134)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = f'C:/Users/sanghui/Desktop/toyproject/DL_Project_5/hsh/data/crop_resize/train/2'\n",
    "\n",
    "max = len(os.listdir(f'{path}/11'))\n",
    "\n",
    "for category_id in os.listdir(path):\n",
    "    if len(os.listdir(f'{path}/{category_id}')) >= max:\n",
    "        max_category_id = category_id\n",
    "\n",
    "max_category_id, len(os.listdir(f'{path}/{max_category_id}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 의류 카테고리중 상품수 가장 많은 파일만일단 학습 시작\n",
    "- 목적 : 해당 카테고리내 샘플링 된 상품수로 클러스터링이 가능한지 확인하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_path = f'C:/Users/sanghui/Desktop/toyproject/DL_Project_5/hsh/data/crop_resize/train/2/{max_category_id}'\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# for i, file in enumerate(os.listdir(max_path)[:25]):\n",
    "#     img_path = max_path + '/' + file\n",
    "#     src = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "#     src = cv2.cvtColor(src, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     plt.subplot(5, 5, i + 1)\n",
    "#     plt.imshow(src)\n",
    "#     plt.title(f'{i+1}: {src.shape} ')\n",
    "#     plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 나중에 전체 데이터셋 훈련시 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(\n",
    "#     zoom_range=[0.5, 0.7], # 이미지 줌\n",
    "#     brightness_range=[0.2, 0.5] # 이미지 밝기\n",
    "# )\n",
    "\n",
    "# test_datagen = ImageDataGenerator(\n",
    "#     zoom_range=[0.5, 0.7],\n",
    "#     brightness_range=[0.2, 0.5]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = train_datagen.flow_from_directory('./data/crop_resize/train/2/39', target_size=(160, 160), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 32\n",
    "# IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3134, 224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = []\n",
    "\n",
    "for image in os.listdir(max_path):\n",
    "    path = max_path + '/' + image\n",
    "    src = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    samples.append(src)\n",
    "\n",
    "samples = np.array(samples)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "batch_size = 32\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained VGG16 model\n",
    "img_shape = (img_height, img_width, 3)\n",
    "VGG16_MODEL = VGG16(include_top=False, weights='imagenet')\n",
    "# customize only return fully connected layer\n",
    "model = Model(inputs=VGG16_MODEL.input, outputs=VGG16_MODEL.get_layer('block5_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = image.img_to_array(samples[0])\n",
    "x = np.expand_dims(samples[0], axis=0)\n",
    "x = preprocess_input(x)\n",
    "# Extract Features with VGG16\n",
    "feature = model.predict(samples)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from the input layer to the last max pooling layer (labeled by 7 x 7 x 512) is regarded as feature extraction part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "# print(consine_loss(features[0], features[1]).numpy())\n",
    "# print(consine_loss(features[0], features[0]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "# consine_loss(feature, feature).numpy()\n",
    "\n",
    "feature / np.linalg.norm(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- -1로 갈수록 유사하고 1로 갈수록 유사하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products_feature_extraction(img):\n",
    "    x = np.expand_dims(samples[0], axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    # Extract Features with VGG16\n",
    "    feature = model.predict(samples)[0]\n",
    "\n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fff8a2ebda7d7c31fff9abc7af7b4b1a85e6e317ed9dea4607b41f7b3e93769e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ds_study': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
